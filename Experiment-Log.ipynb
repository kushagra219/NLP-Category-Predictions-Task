{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Task Experiment Log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"All experiments are learning EXPERIENCES, when they do not go as PLANNED\" - Paige Hudson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date - 03 Apr 2021\n",
    "\n",
    "## Log Entries -\n",
    "\n",
    "* This is a classic text classification problem of NLP. \n",
    "* We have to predict the primary product category based on product description mainly. (would test on other features to see if test accuracy gets improved or not).\n",
    "* Also, I have to extract the primary category from the product category tree column. (which is clearly the first entry in the product cateogry tree)\n",
    "* I'll try to visualize the frequency of all primary categories and remove the least frequent ones. \n",
    "* To preprocess the description, I would have to remove the stop words and punctuation marks to start with. And later, I am thinking of taking root word for each word in the description of each row. (e.g. played and playing).\n",
    "* Instead of making a 2D matrix manually by calculating frequency using dictionary, I'll use <strong>CountVectorizer</strong> from <strong>sklearn.feature_extraction.text</strong> to get the 2D numpy frequency array directly from the cleaned description column. (save time and avoid mistakes)\n",
    "* I will split the train and test data using train_test_data using sklearn.model_selection\n",
    "* Then, I would use common algorithms from sklearn and train every classifier on the training data for description and primary category. Later, I also plan to use neural networks models. \n",
    "* Finally, I would analyse the test acurracy, F1-score etc of each algorithm (using classficiation report and confusion matrix) from sklearn.metrics to find out which algorithm works the best. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date - 04 Apr 2021\n",
    "\n",
    "\n",
    "## Log Entries \n",
    "\n",
    "* Today, I plan to do some reseach on cleaning of text data. (read articles/explore documentation of NLTK)\n",
    "* After doing the data cleaning and preprocessing again on the data, I will lookup better and more visualizations for our data and implement them.\n",
    "* Though I achieved a good accuracy the previous day, I will GridSearchCV to tune the hyperparameters of my current models to get even better accuracy.\n",
    "* I will try advanced algorithms in sklearn and compare its accuracy with the algorithms used until now.\n",
    "* I will try to explore other metrics to analyse the performance my models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date - 05 Apr 2021\n",
    "\n",
    "\n",
    "## Log Entries \n",
    "\n",
    "* I tried different data cleaning and exploration techniques such as n gram exploration, named entity recognition to get insights into the data\n",
    "* Top bi-grams features gives generic terms, so they cannot be used as features, so I added generic terms to stopwords\n",
    "* NER also revealed that cardinals occur very freqently, so they may be used a feature \n",
    "* After Exploraration, I clean the words using WordNetLemmatizer, spell check etc.  \n",
    "* Used wordcloud to visualize the cleaned words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date - 06 Apr 2021\n",
    "\n",
    "\n",
    "## Log Entries \n",
    "\n",
    "* Machine Learning models from sklearn were trained on the clean dataset\n",
    "* KNeigbours did not perform well on this data\n",
    "* Xgboost and Random Forst Classifier gave really promising results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date - 07 Apr 2021\n",
    "\n",
    "\n",
    "## Log Entries  \n",
    "\n",
    "* Some ML models (Multinomial Naive Bayes, Random Forst) are giving good accuraacies around 95-96%. \n",
    "* High performance ensemble models such as Xgboost and Graident Boosting Trees generally given good accuracies, so have to try that out. \n",
    "* Tried implementing them, and Xgboost worked really well giving 97.4% accuracy on testing data and gradient boosting trees also performed well but they weere overfitting the training set.\n",
    "* I tried out different hyperparameters for Gradient Boosting trees but it is taking a lot of time to train. \n",
    "* I pickled all the time consuming models so I can load them, when they run again. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date - 08 Apr 2021\n",
    "\n",
    "\n",
    "## Log Entries  \n",
    "\n",
    "* As ML models gave suffieciently good results, but deep learning models work even better, so have to try that out. \n",
    "* I will try to implement DNN, RNN, CNN layers to find out which performs the best \n",
    "* Implemenation of Deep Neural Network with embedding layer and with Count / TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date - 09 Apr 2021\n",
    "\n",
    "\n",
    "## Log Entries \n",
    "\n",
    "* Tried reading the documenation for BERT and XLSNet and it was using Deep Learning at its core. \n",
    "* Tried coding and implementing BERT but I didn't get most of what it was trying to do. \n",
    "* They give higher accuracy when fine tuned, but it will take some time to see how they work. \n",
    "* Nevertheless, I prepared the report of my whole task today. \n",
    "* I worked hard on learning and implementing whatever I could, so fingers crossed :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Citations\n",
    "\n",
    "* https://neptune.ai/blog/text-classification-tips-and-tricks-kaggle-competitions\n",
    "* https://neptune.ai/blog/exploratory-data-analysis-natural-language-processing-tools\n",
    "* https://www.analyticsvidhya.com/blog/2014/11/text-data-cleaning-steps-python/\n",
    "* https://www.analyticsvidhya.com/blog/2016/08/beginners-guide-to-topic-modeling-in-python/\n",
    "* https://www.analyticsvidhya.com/blog/2017/01/ultimate-guide-to-understand-implement-natural-language-processing-codes-in-python/\n",
    "* https://pub.towardsai.net/natural-language-processing-nlp-with-python-tutorial-for-beginners-1f54e610a1a0\n",
    "* https://www.machinelearningplus.com/nlp/natural-language-processing-guide/\n",
    "* https://towardsdatascience.com/ml-powered-product-categorization-for-smart-shopping-options-8f10d78e3294\n",
    "* https://www.analyticsvidhya.com/blog/2017/01/ultimate-guide-to-understand-implement-natural-language-processing-codes-in-python/\n",
    "* https://medium.com/text-classification-algorithms/text-classification-algorithms-a-survey-a215b7ab7e2d\n",
    "* https://machinelearningmastery.com/xgboost-for-imbalanced-classification/#:~:text=The%20XGBoost%20algorithm%20is%20effective,over%20the%20model%20training%20procedure.\n",
    "* https://medium.datadriveninvestor.com/deep-learning-techniques-for-text-classification-9392ca9492c7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
